{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Computation Graph and work out the gradient dz/da by following the path\n",
    "back from z to a and compare the result with the analytical gradient.\n",
    "\n",
    "x = 2*a + 3*b\n",
    "\n",
    "y = 5* a * a + 3*b *b *b\n",
    "\n",
    "z = 2*x + 3*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z with respect to a:  tensor(64.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "x = 2 * a + 3 * b\n",
    "y = 5 * a**2 + 3 * b**3\n",
    "z = 2 * x + 3 * y\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradient of z with respect to a: \", a.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following Computation Graph, work out the gradient da/dw by following the\n",
    "path back from a to w and compare the result with the analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(4., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "b=torch.tensor(2.0, requires_grad=True)\n",
    "x=torch.tensor(4.0, requires_grad=True)\n",
    "w=torch.tensor(6.0, requires_grad=True)\n",
    "def relu(x):\n",
    "  if x>0:\n",
    "    return x\n",
    "  else:\n",
    "    return 0  \n",
    "def cal(w,x,b):\n",
    "  v=w*x+b\n",
    "  v=relu(v)\n",
    "  return x\n",
    "  \n",
    "u=w*x\n",
    "v=u+b\n",
    "a=relu(v)\n",
    "a.backward()\n",
    "print(w.grad)\n",
    "print(cal(w,x,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the Problem 2 using Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch computed gradient of a with respect to w: 0.09531199187040329\n",
      "Manual computed gradient of a with respect to w: 0.09531199187040329\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "def manual_gradient_w(w, x, b):\n",
    "    v = w * x + b  \n",
    "    sig = sigmoid(v)  \n",
    "    grad_w = sig * (1 - sig) * x  \n",
    "    return grad_w\n",
    "\n",
    "\n",
    "w = torch.tensor(0.6, requires_grad=True)\n",
    "x = torch.tensor(0.4, requires_grad=True)\n",
    "b = torch.tensor(0.2, requires_grad=True)\n",
    "\n",
    "\n",
    "v = w * x + b\n",
    "a = sigmoid(v)\n",
    "\n",
    "\n",
    "a.backward()\n",
    "\n",
    "\n",
    "grad_w_manual = manual_gradient_w(w, x, b)\n",
    "\n",
    "\n",
    "print(f\"PyTorch computed gradient of a with respect to w: {w.grad.item()}\")\n",
    "print(f\"Manual computed gradient of a with respect to w: {grad_w_manual.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the gradients provided by PyTorch match with the analytical gradients of\n",
    "the function f= exp(-x2-2x-sin(x)) w.r.t x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch computed gradient: -0.09744400531053543\n",
      "Analytical computed gradient: -0.09744400531053543\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def f(x):\n",
    "    return torch.exp(-x**2 - 2*x - torch.sin(x))\n",
    "\n",
    "def analytical_gradient(x):\n",
    "    return torch.exp(-x**2 - 2*x - torch.sin(x)) * (-2*x - 2 - torch.cos(x))\n",
    "\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)  \n",
    "\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "\n",
    "y.backward()\n",
    "\n",
    "\n",
    "pytorch_gradient = x.grad.item()\n",
    "\n",
    "analytical_grad = analytical_gradient(x).item()\n",
    "\n",
    "# Output the results\n",
    "print(f\"PyTorch computed gradient: {pytorch_gradient}\")\n",
    "print(f\"Analytical computed gradient: {analytical_grad}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient for the function y=8x4+ 3x3 +7x2+6x+3 and verify the gradients\n",
    "provided by PyTorch with the analytical gradients. A snapshot of the Python code is\n",
    "provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(326.)\n",
      "Analytical computed gradient: 326.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def y_function(x):\n",
    "    return 8*x**4 + 3*x**3 + 7*x**2 + 6*x + 3\n",
    "\n",
    "def analytical_gradient(x):\n",
    "    return 32*x**3 + 9*x**2 + 14*x + 6\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)  \n",
    "y = y_function(x)\n",
    "y.backward()\n",
    "pytorch_gradient = x.grad\n",
    "\n",
    "analytical_grad = analytical_gradient(x)\n",
    "\n",
    "# Output the results\n",
    "print(x.grad)\n",
    "print(f\"Analytical computed gradient: {analytical_grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the intermediate variables a,b,c,d, and e in the forward pass. Starting from f,\n",
    "calculate the gradient of each expression in the backward pass manually. Calculate ∂f/∂y\n",
    "using the computational graph and chain rule. Use the chain rule to calculate gradient and\n",
    "compare with analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch gradient:  -0.005758530460298061\n",
      "manual gradient:  -0.005758530460298061\n"
     ]
    }
   ],
   "source": [
    "def calc_grad(x, y, z, a, b, c, d, e, f):\n",
    "    dfde = 1-torch.tanh(e)**2\n",
    "    dedd = 1/(d+1); dddc = z\n",
    "    dcdb = -1*a/b**2\n",
    "    dbdy = torch.cos(y)\n",
    "    dfdy = dfde*dedd*dddc*dcdb*dbdy\n",
    "    return dfdy.item()\n",
    "\n",
    "x = torch.tensor(2.6, requires_grad=True)\n",
    "y = torch.tensor(6.5, requires_grad=True)\n",
    "z = torch.tensor(2.26, requires_grad=True)\n",
    "\n",
    "a = 2*x\n",
    "b = torch.sin(y)\n",
    "c = a/b\n",
    "d = z*c\n",
    "e = torch.log(d+1)\n",
    "f = torch.tanh(e)\n",
    "\n",
    "f.backward()\n",
    "print(\"torch gradient: \", y.grad.item())\n",
    "print(\"manual gradient: \", calc_grad(x, y, z, a, b, c, d, e, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
